{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1253a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bisha\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daf1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) =mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78725a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa06fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d81f6f6230>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train[0],cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3050d781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc4a4962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bisha\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669706 (2.55 MB)\n",
      "Trainable params: 669706 (2.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\bisha\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bisha\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2765 - accuracy: 0.9181\n",
      "Epoch 1: val_loss improved from inf to 0.12330, saving model to best_model.keras\n",
      "375/375 [==============================] - 8s 18ms/step - loss: 0.2757 - accuracy: 0.9183 - val_loss: 0.1233 - val_accuracy: 0.9622 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9664\n",
      "Epoch 2: val_loss improved from 0.12330 to 0.08884, saving model to best_model.keras\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.1102 - accuracy: 0.9664 - val_loss: 0.0888 - val_accuracy: 0.9722 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9763\n",
      "Epoch 3: val_loss improved from 0.08884 to 0.08476, saving model to best_model.keras\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.0758 - accuracy: 0.9763 - val_loss: 0.0848 - val_accuracy: 0.9743 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9800\n",
      "Epoch 4: val_loss improved from 0.08476 to 0.08010, saving model to best_model.keras\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0620 - accuracy: 0.9800 - val_loss: 0.0801 - val_accuracy: 0.9757 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9853\n",
      "Epoch 5: val_loss did not improve from 0.08010\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0464 - accuracy: 0.9853 - val_loss: 0.1021 - val_accuracy: 0.9723 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9870\n",
      "Epoch 6: val_loss improved from 0.08010 to 0.07932, saving model to best_model.keras\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.0385 - accuracy: 0.9869 - val_loss: 0.0793 - val_accuracy: 0.9797 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9884\n",
      "Epoch 7: val_loss did not improve from 0.07932\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0350 - accuracy: 0.9885 - val_loss: 0.0876 - val_accuracy: 0.9754 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9898\n",
      "Epoch 8: val_loss did not improve from 0.07932\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.0882 - val_accuracy: 0.9768 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9901\n",
      "Epoch 9: val_loss did not improve from 0.07932\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.0285 - accuracy: 0.9901 - val_loss: 0.0872 - val_accuracy: 0.9773 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9954\n",
      "Epoch 10: val_loss improved from 0.07932 to 0.07345, saving model to best_model.keras\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0735 - val_accuracy: 0.9817 - lr: 2.0000e-04\n",
      "Epoch 11/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9979\n",
      "Epoch 11: val_loss did not improve from 0.07345\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.0773 - val_accuracy: 0.9830 - lr: 2.0000e-04\n",
      "Epoch 12/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9979\n",
      "Epoch 12: val_loss did not improve from 0.07345\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0762 - val_accuracy: 0.9834 - lr: 2.0000e-04\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9988\n",
      "Epoch 13: val_loss did not improve from 0.07345\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0788 - val_accuracy: 0.9835 - lr: 2.0000e-04\n",
      "Epoch 14/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9988\n",
      "Epoch 14: val_loss did not improve from 0.07345\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0793 - val_accuracy: 0.9835 - lr: 4.0000e-05\n",
      "Epoch 15/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 15: val_loss did not improve from 0.07345\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0795 - val_accuracy: 0.9835 - lr: 4.0000e-05\n",
      "After training, test accuracy is 98.40999841690063\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# Convert labels to one hot vectors\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))  # Preventing overfitting\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Create callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='best_model.keras', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=50, \n",
    "                    validation_split=0.2, callbacks=[checkpointer, reduce_lr, early_stopping], \n",
    "                    verbose=1, shuffle=True)\n",
    "\n",
    "# Load the best model after training\n",
    "model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Evaluate accuracy after training\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print(\"After training, test accuracy is\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "636afc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Recognized Alphabets: 55239715331235256278390558575239625780959521123844444949949\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define color boundaries for blue\n",
    "blueLower = np.array([70, 50, 50])\n",
    "blueUpper = np.array([90, 255, 255])\n",
    "\n",
    "# Create kernel for morphological transformations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Initialize deque to store points\n",
    "points = deque(maxlen=512)\n",
    "\n",
    "# Initialize blackboard to draw on\n",
    "blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "\n",
    "# Load your model\n",
    "model = tf.keras.models.load_model('best_model1.keras')  # Replace 'your_model_path.h5' with your model path\n",
    "\n",
    "# Define letters (adjust this as per your model's output)\n",
    "letters = {0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9'}\n",
    "\n",
    "# Initialize a list to store recognized alphabets\n",
    "recognized_alphabets = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "prediction = 26\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "     \n",
    "    # Detecting which pixel value falls under blue color boundaries\n",
    "    blue = cv2.inRange(hsv, blueLower, blueUpper)\n",
    "     \n",
    "    # Erosion, opening, and dilation\n",
    "    blue = cv2.erode(blue, kernel)\n",
    "    blue = cv2.morphologyEx(blue, cv2.MORPH_OPEN, kernel)\n",
    "    blue = cv2.dilate(blue, kernel)\n",
    "     \n",
    "    # Find contours in the image\n",
    "    cnts, _ = cv2.findContours(blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "      \n",
    "    center = None\n",
    "     \n",
    "    # If any contours were found\n",
    "    if len(cnts) > 0:\n",
    "        cnt = sorted(cnts, key=cv2.contourArea, reverse=True)[0]\n",
    "        ((x, y), radius) = cv2.minEnclosingCircle(cnt)\n",
    "        cv2.circle(frame, (int(x), int(y)), int(radius), (125, 344, 278), 2)\n",
    "         \n",
    "        M = cv2.moments(cnt)\n",
    "        center = (int(M['m10'] / M['m00']), int(M['m01'] / M['m00']))\n",
    "     \n",
    "        points.appendleft(center)\n",
    "         \n",
    "    elif len(cnts) == 0:\n",
    "        if len(points) != 0:\n",
    "            blackboard_gray = cv2.cvtColor(blackboard, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.medianBlur(blackboard_gray, 15)\n",
    "            blur = cv2.GaussianBlur(blur, (5, 5), 0)\n",
    "            thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "            cv2.imshow(\"Thresh\", thresh)\n",
    "             \n",
    "            blackboard_cnts, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "             \n",
    "            if len(blackboard_cnts) >= 1:\n",
    "                cnt = sorted(blackboard_cnts, key=cv2.contourArea, reverse=True)[0]\n",
    "                 \n",
    "                if cv2.contourArea(cnt) > 1000:\n",
    "                    x, y, w, h = cv2.boundingRect(cnt)\n",
    "                    alphabet = blackboard_gray[y-10:y+h+10, x-10:x+w+10]\n",
    "                    try:\n",
    "                        img = cv2.resize(alphabet, (28, 28))\n",
    "                    except cv2.error as e:\n",
    "                        continue\n",
    "                     \n",
    "                    img = np.array(img)\n",
    "                    img = img.astype('float32') / 255\n",
    "                     \n",
    "                    prediction = model.predict(img.reshape(1, 28, 28))[0]\n",
    "                    prediction = np.argmax(prediction)\n",
    "                    \n",
    "                    # Append the recognized alphabet to the list\n",
    "                    recognized_alphabets.append(letters[prediction])\n",
    "                     \n",
    "            # Empty the point deque and also blackboard\n",
    "            points = deque(maxlen=512)\n",
    "            blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "         \n",
    "    # Connect the detected points with a line\n",
    "    for i in range(1, len(points)):\n",
    "        if points[i-1] is None or points[i] is None:\n",
    "            continue\n",
    "        cv2.line(frame, points[i-1], points[i], (0, 0, 0), 2)\n",
    "        cv2.line(blackboard, points[i-1], points[i], (255, 255, 255), 8)\n",
    "         \n",
    "    # Ensure the prediction is within the valid range\n",
    "    if 0 <= prediction < len(letters):\n",
    "        cv2.putText(frame, \"Prediction: \" + str(letters[prediction]), (20, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "     \n",
    "    cv2.imshow(\"Alphabet Recognition System\", frame)\n",
    "     \n",
    "    if cv2.waitKey(1) == 13:  # if I press enter\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Join the recognized alphabets into a single string and print it\n",
    "recognized_string = ''.join(recognized_alphabets)\n",
    "print(\"Recognized Alphabets:\", recognized_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f397c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261cefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccdaa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9d3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45599426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992a42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c83877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcb602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b8e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74bee708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669706 (2.55 MB)\n",
      "Trainable params: 669706 (2.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.9101 - accuracy: 0.8932\n",
      "Epoch 1: val_loss improved from inf to 0.49319, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 19ms/step - loss: 0.9073 - accuracy: 0.8935 - val_loss: 0.4932 - val_accuracy: 0.9563 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.4431 - accuracy: 0.9477\n",
      "Epoch 2: val_loss improved from 0.49319 to 0.33642, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.4424 - accuracy: 0.9479 - val_loss: 0.3364 - val_accuracy: 0.9613 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.9552\n",
      "Epoch 3: val_loss improved from 0.33642 to 0.28353, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.3378 - accuracy: 0.9552 - val_loss: 0.2835 - val_accuracy: 0.9668 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.9578\n",
      "Epoch 4: val_loss improved from 0.28353 to 0.27259, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.3095 - accuracy: 0.9578 - val_loss: 0.2726 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.9596\n",
      "Epoch 5: val_loss improved from 0.27259 to 0.26219, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.2973 - accuracy: 0.9595 - val_loss: 0.2622 - val_accuracy: 0.9686 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9604\n",
      "Epoch 6: val_loss improved from 0.26219 to 0.25052, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.2878 - accuracy: 0.9604 - val_loss: 0.2505 - val_accuracy: 0.9714 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9606\n",
      "Epoch 7: val_loss improved from 0.25052 to 0.24644, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.2853 - accuracy: 0.9606 - val_loss: 0.2464 - val_accuracy: 0.9728 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.9614\n",
      "Epoch 8: val_loss did not improve from 0.24644\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.2822 - accuracy: 0.9614 - val_loss: 0.2490 - val_accuracy: 0.9714 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.9610\n",
      "Epoch 9: val_loss did not improve from 0.24644\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.2807 - accuracy: 0.9610 - val_loss: 0.2494 - val_accuracy: 0.9697 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2762 - accuracy: 0.9613\n",
      "Epoch 10: val_loss did not improve from 0.24644\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.2760 - accuracy: 0.9614 - val_loss: 0.2542 - val_accuracy: 0.9697 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.9765\n",
      "Epoch 11: val_loss improved from 0.24644 to 0.20082, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.2225 - accuracy: 0.9765 - val_loss: 0.2008 - val_accuracy: 0.9776 - lr: 2.0000e-04\n",
      "Epoch 12/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9796\n",
      "Epoch 12: val_loss improved from 0.20082 to 0.18111, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1917 - accuracy: 0.9796 - val_loss: 0.1811 - val_accuracy: 0.9798 - lr: 2.0000e-04\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9791\n",
      "Epoch 13: val_loss improved from 0.18111 to 0.17082, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1770 - accuracy: 0.9791 - val_loss: 0.1708 - val_accuracy: 0.9800 - lr: 2.0000e-04\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9816\n",
      "Epoch 14: val_loss improved from 0.17082 to 0.16286, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.1639 - accuracy: 0.9816 - val_loss: 0.1629 - val_accuracy: 0.9782 - lr: 2.0000e-04\n",
      "Epoch 15/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9818\n",
      "Epoch 15: val_loss improved from 0.16286 to 0.16165, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.1573 - accuracy: 0.9819 - val_loss: 0.1617 - val_accuracy: 0.9775 - lr: 2.0000e-04\n",
      "Epoch 16/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9808\n",
      "Epoch 16: val_loss improved from 0.16165 to 0.15487, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.1542 - accuracy: 0.9808 - val_loss: 0.1549 - val_accuracy: 0.9793 - lr: 2.0000e-04\n",
      "Epoch 17/50\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9811\n",
      "Epoch 17: val_loss did not improve from 0.15487\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.1504 - accuracy: 0.9812 - val_loss: 0.1567 - val_accuracy: 0.9787 - lr: 2.0000e-04\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9820\n",
      "Epoch 18: val_loss improved from 0.15487 to 0.15052, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1456 - accuracy: 0.9820 - val_loss: 0.1505 - val_accuracy: 0.9790 - lr: 2.0000e-04\n",
      "Epoch 19/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.9828\n",
      "Epoch 19: val_loss did not improve from 0.15052\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1420 - accuracy: 0.9828 - val_loss: 0.1507 - val_accuracy: 0.9803 - lr: 2.0000e-04\n",
      "Epoch 20/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9828\n",
      "Epoch 20: val_loss improved from 0.15052 to 0.14637, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 0.1416 - accuracy: 0.9828 - val_loss: 0.1464 - val_accuracy: 0.9800 - lr: 2.0000e-04\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9835\n",
      "Epoch 21: val_loss improved from 0.14637 to 0.14451, saving model to best_model1.keras\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.1366 - accuracy: 0.9835 - val_loss: 0.1445 - val_accuracy: 0.9796 - lr: 2.0000e-04\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9831\n",
      "Epoch 22: val_loss improved from 0.14451 to 0.14200, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1353 - accuracy: 0.9831 - val_loss: 0.1420 - val_accuracy: 0.9806 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9830\n",
      "Epoch 23: val_loss improved from 0.14200 to 0.14066, saving model to best_model1.keras\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.1342 - accuracy: 0.9830 - val_loss: 0.1407 - val_accuracy: 0.9801 - lr: 2.0000e-04\n",
      "Epoch 24/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9837\n",
      "Epoch 24: val_loss did not improve from 0.14066\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1312 - accuracy: 0.9837 - val_loss: 0.1410 - val_accuracy: 0.9808 - lr: 2.0000e-04\n",
      "Epoch 25/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9840\n",
      "Epoch 25: val_loss improved from 0.14066 to 0.13990, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1295 - accuracy: 0.9840 - val_loss: 0.1399 - val_accuracy: 0.9803 - lr: 2.0000e-04\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9840\n",
      "Epoch 26: val_loss improved from 0.13990 to 0.13854, saving model to best_model1.keras\n",
      "375/375 [==============================] - 10s 25ms/step - loss: 0.1278 - accuracy: 0.9840 - val_loss: 0.1385 - val_accuracy: 0.9794 - lr: 2.0000e-04\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9843\n",
      "Epoch 27: val_loss did not improve from 0.13854\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1263 - accuracy: 0.9843 - val_loss: 0.1386 - val_accuracy: 0.9812 - lr: 2.0000e-04\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9846\n",
      "Epoch 28: val_loss improved from 0.13854 to 0.13576, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1239 - accuracy: 0.9846 - val_loss: 0.1358 - val_accuracy: 0.9810 - lr: 2.0000e-04\n",
      "Epoch 29/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9847\n",
      "Epoch 29: val_loss improved from 0.13576 to 0.13526, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1244 - accuracy: 0.9847 - val_loss: 0.1353 - val_accuracy: 0.9804 - lr: 2.0000e-04\n",
      "Epoch 30/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9850\n",
      "Epoch 30: val_loss improved from 0.13526 to 0.13474, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1230 - accuracy: 0.9850 - val_loss: 0.1347 - val_accuracy: 0.9803 - lr: 2.0000e-04\n",
      "Epoch 31/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9847\n",
      "Epoch 31: val_loss improved from 0.13474 to 0.13388, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1224 - accuracy: 0.9847 - val_loss: 0.1339 - val_accuracy: 0.9807 - lr: 2.0000e-04\n",
      "Epoch 32/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9857\n",
      "Epoch 32: val_loss did not improve from 0.13388\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1207 - accuracy: 0.9857 - val_loss: 0.1342 - val_accuracy: 0.9805 - lr: 2.0000e-04\n",
      "Epoch 33/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9857\n",
      "Epoch 33: val_loss improved from 0.13388 to 0.12969, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1198 - accuracy: 0.9857 - val_loss: 0.1297 - val_accuracy: 0.9818 - lr: 2.0000e-04\n",
      "Epoch 34/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9863\n",
      "Epoch 34: val_loss did not improve from 0.12969\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1170 - accuracy: 0.9864 - val_loss: 0.1347 - val_accuracy: 0.9803 - lr: 2.0000e-04\n",
      "Epoch 35/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9857\n",
      "Epoch 35: val_loss did not improve from 0.12969\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1191 - accuracy: 0.9856 - val_loss: 0.1347 - val_accuracy: 0.9793 - lr: 2.0000e-04\n",
      "Epoch 36/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9862\n",
      "Epoch 36: val_loss improved from 0.12969 to 0.12870, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1176 - accuracy: 0.9861 - val_loss: 0.1287 - val_accuracy: 0.9812 - lr: 2.0000e-04\n",
      "Epoch 37/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9865\n",
      "Epoch 37: val_loss did not improve from 0.12870\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1157 - accuracy: 0.9865 - val_loss: 0.1301 - val_accuracy: 0.9804 - lr: 2.0000e-04\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9859\n",
      "Epoch 38: val_loss did not improve from 0.12870\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1163 - accuracy: 0.9859 - val_loss: 0.1321 - val_accuracy: 0.9811 - lr: 2.0000e-04\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9860\n",
      "Epoch 39: val_loss did not improve from 0.12870\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1140 - accuracy: 0.9860 - val_loss: 0.1292 - val_accuracy: 0.9810 - lr: 2.0000e-04\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9900\n",
      "Epoch 40: val_loss improved from 0.12870 to 0.12377, saving model to best_model1.keras\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.1030 - accuracy: 0.9900 - val_loss: 0.1238 - val_accuracy: 0.9828 - lr: 4.0000e-05\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9914\n",
      "Epoch 41: val_loss improved from 0.12377 to 0.12109, saving model to best_model1.keras\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0994 - accuracy: 0.9914 - val_loss: 0.1211 - val_accuracy: 0.9828 - lr: 4.0000e-05\n",
      "Epoch 42/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9911\n",
      "Epoch 42: val_loss improved from 0.12109 to 0.11996, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0985 - accuracy: 0.9911 - val_loss: 0.1200 - val_accuracy: 0.9833 - lr: 4.0000e-05\n",
      "Epoch 43/50\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9917\n",
      "Epoch 43: val_loss improved from 0.11996 to 0.11938, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0960 - accuracy: 0.9918 - val_loss: 0.1194 - val_accuracy: 0.9829 - lr: 4.0000e-05\n",
      "Epoch 44/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9922\n",
      "Epoch 44: val_loss improved from 0.11938 to 0.11797, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0949 - accuracy: 0.9921 - val_loss: 0.1180 - val_accuracy: 0.9833 - lr: 4.0000e-05\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9914\n",
      "Epoch 45: val_loss improved from 0.11797 to 0.11743, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0942 - accuracy: 0.9914 - val_loss: 0.1174 - val_accuracy: 0.9829 - lr: 4.0000e-05\n",
      "Epoch 46/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9917\n",
      "Epoch 46: val_loss improved from 0.11743 to 0.11710, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0939 - accuracy: 0.9917 - val_loss: 0.1171 - val_accuracy: 0.9829 - lr: 4.0000e-05\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9915\n",
      "Epoch 47: val_loss did not improve from 0.11710\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0932 - accuracy: 0.9915 - val_loss: 0.1172 - val_accuracy: 0.9835 - lr: 4.0000e-05\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9919\n",
      "Epoch 48: val_loss improved from 0.11710 to 0.11603, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0924 - accuracy: 0.9919 - val_loss: 0.1160 - val_accuracy: 0.9829 - lr: 4.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9913\n",
      "Epoch 49: val_loss improved from 0.11603 to 0.11481, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.0921 - accuracy: 0.9913 - val_loss: 0.1148 - val_accuracy: 0.9830 - lr: 4.0000e-05\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9912\n",
      "Epoch 50: val_loss improved from 0.11481 to 0.11456, saving model to best_model1.keras\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0921 - accuracy: 0.9912 - val_loss: 0.1146 - val_accuracy: 0.9831 - lr: 4.0000e-05\n",
      "After training, test accuracy is 98.4000027179718\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# Convert labels to one hot vectors\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "model.add(Dropout(0.4))  # Increased dropout rate\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))  # Added L2 regularization\n",
    "model.add(Dropout(0.4))  # Increased dropout rate\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Create callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='best_model1.keras', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=50, \n",
    "                    validation_split=0.2, callbacks=[checkpointer, reduce_lr, early_stopping], \n",
    "                    verbose=1, shuffle=True)\n",
    "\n",
    "# Load the best model after training\n",
    "model = tf.keras.models.load_model('best_model1.keras')\n",
    "\n",
    "# Evaluate accuracy after training\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "print(\"After training, test accuracy is\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be392367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
